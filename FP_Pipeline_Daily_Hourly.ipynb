{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "### Installing Relevant Packages"
      ],
      "metadata": {
        "id": "wViT86YnPP3J"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bcDbqeNPIy9C"
      },
      "outputs": [],
      "source": [
        "#pip install yfinance\n",
        "#!pip install sqlalchemy psycopg2-binary yfinance pandas"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "###\n",
        "### Importing Libraries\n"
      ],
      "metadata": {
        "id": "zcHZ5sDkPUtA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from datetime import datetime, timedelta\n",
        "\n",
        "import pandas as pd\n",
        "import yfinance as yf\n",
        "from sqlalchemy import create_engine, text"
      ],
      "metadata": {
        "id": "BlijjCeDJpL8"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "###\n",
        "### Defining Configurations"
      ],
      "metadata": {
        "id": "5afbylJ5Pdff"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ===================== CONFIG =====================\n",
        "\n",
        "DATABASE_URL = 'postgresql://neondb_owner:npg_pw2QhB4XGqbx@ep-muddy-union-ad4fjgcw-pooler.c-2.us-east-1.aws.neon.tech/neondb?sslmode=require&channel_binding=require'\n",
        "\n",
        "# 2) Ticker + interval settings\n",
        "TICKER = \"TCS.NS\"            # change this to your chosen company\n",
        "HOURLY_INTERVAL = \"60m\"      # 60-minute bars\n",
        "\n",
        "# Create SQLAlchemy engine\n",
        "#engine = create_engine(DATABASE_URL, echo=False, future=True)\n",
        "engine = create_engine(\n",
        "    DATABASE_URL,\n",
        "    echo=False,\n",
        "    future=True,\n",
        "    pool_pre_ping=True,    # ‚úÖ checks connection before using it\n",
        "    pool_recycle=300,      # ‚úÖ recycle conns every 5 minutes\n",
        "    pool_size=5,\n",
        "    max_overflow=5,\n",
        ")"
      ],
      "metadata": {
        "id": "TM2yXzk8UKx3"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "###\n",
        "### Database Set-up Function Definition\n",
        "- Function call will be commented after one time execution"
      ],
      "metadata": {
        "id": "mtWJ7jPTPiPp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ===================== DB SETUP =====================\n",
        "\n",
        "def init_tables():\n",
        "    \"\"\"\n",
        "    Create raw daily & hourly tables in the cloud Postgres DB if they don't exist.\n",
        "    \"\"\"\n",
        "    create_daily_sql = \"\"\"\n",
        "    CREATE TABLE IF NOT EXISTS raw_daily_prices (\n",
        "        ticker      VARCHAR(20) NOT NULL,\n",
        "        date        DATE        NOT NULL,\n",
        "        open        DOUBLE PRECISION,\n",
        "        high        DOUBLE PRECISION,\n",
        "        low         DOUBLE PRECISION,\n",
        "        close       DOUBLE PRECISION,\n",
        "        adj_close   DOUBLE PRECISION,\n",
        "        volume      BIGINT,\n",
        "        PRIMARY KEY (ticker, date)\n",
        "    );\n",
        "    \"\"\"\n",
        "\n",
        "    create_hourly_sql = \"\"\"\n",
        "    CREATE TABLE IF NOT EXISTS raw_hourly_prices (\n",
        "        ticker      VARCHAR(20) NOT NULL,\n",
        "        ts          TIMESTAMP   NOT NULL,\n",
        "        interval    VARCHAR(10) NOT NULL,\n",
        "        open        DOUBLE PRECISION,\n",
        "        high        DOUBLE PRECISION,\n",
        "        low         DOUBLE PRECISION,\n",
        "        close       DOUBLE PRECISION,\n",
        "        adj_close   DOUBLE PRECISION,\n",
        "        volume      BIGINT,\n",
        "        PRIMARY KEY (ticker, ts, interval)\n",
        "    );\n",
        "    \"\"\"\n",
        "\n",
        "    with engine.begin() as conn:\n",
        "        conn.execute(text(create_daily_sql))\n",
        "        conn.execute(text(create_hourly_sql))\n",
        "\n",
        "    print(\"‚úÖ Tables ensured in cloud Postgres (raw_daily_prices, raw_hourly_prices).\")\n",
        "\n"
      ],
      "metadata": {
        "id": "Do9NPxl5UgGv"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "###\n",
        "### One Time Daily Backfill for last ten years Function Definition\n",
        "- Function call will be commented after one time execution"
      ],
      "metadata": {
        "id": "jCc7R7FBP036"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ===================== ONE-TIME DAILY BACKFILL =====================\n",
        "\n",
        "def backfill_daily(years_back: int = 10):\n",
        "    \"\"\"\n",
        "    Download ~N years of DAILY OHLCV data from Yahoo Finance\n",
        "    and store it in raw_daily_prices (ON CONFLICT DO NOTHING).\n",
        "\n",
        "    Run this ONCE at the beginning, or when you want to refresh history.\n",
        "    \"\"\"\n",
        "\n",
        "    # Fix for DeprecationWarning: datetime.utcnow() and AttributeError: type object 'datetime.datetime' has no attribute 'UTC'\n",
        "    end_date = datetime.now(timezone.utc).date() # Use timezone.utc\n",
        "    start_date = end_date - timedelta(days=365 * years_back)\n",
        "\n",
        "    print(f\"üì• Downloading daily data for {TICKER} from {start_date} to {end_date}...\")\n",
        "    # Fix for KeyError: \"['Adj Close'] not in index\"\n",
        "    # Set auto_adjust=False and prepost=False to ensure 'Adj Close' column is present\n",
        "    df_daily = yf.download(TICKER, start=start_date, end=end_date, interval=\"1d\", auto_adjust=False, prepost=False)\n",
        "\n",
        "    if df_daily.empty:\n",
        "        print(\"‚ö†Ô∏è No daily data returned from Yahoo.\")\n",
        "        return\n",
        "\n",
        "    # --- Robust Column Handling --- Fix for KeyError: 'date' and general column consistency ---\n",
        "\n",
        "    # 1. Standardize MultiIndex columns if they exist: convert ('Open', '') to 'Open'\n",
        "    if isinstance(df_daily.columns, pd.MultiIndex):\n",
        "        # Use droplevel(1) to get single-level column names like 'Open', 'High', 'Adj Close', 'Volume'\n",
        "        df_daily.columns = df_daily.columns.droplevel(1)\n",
        "\n",
        "    # 2. Reset index to bring the Date (and potentially Ticker if MultiIndex index) from index to a column\n",
        "    # After droplevel(1), the index name should be 'Date' if it was part of original yf.download structure.\n",
        "    df_daily = df_daily.reset_index() # This will create a 'Date' column\n",
        "\n",
        "    # 3. Rename all columns to desired database-friendly lowercase names in one go\n",
        "    df_daily.rename(\n",
        "        columns={\n",
        "            \"Date\": \"date\", # Always ensure 'Date' becomes 'date'\n",
        "            \"Open\": \"open\",\n",
        "            \"High\": \"high\",\n",
        "            \"Low\": \"low\",\n",
        "            \"Close\": \"close\",\n",
        "            \"Adj Close\": \"adj_close\",\n",
        "            \"Volume\": \"volume\",\n",
        "            \"Ticker\": \"ticker\" # In case 'Ticker' also comes from MultiIndex index after reset_index\n",
        "        },\n",
        "        inplace=True,\n",
        "    )\n",
        "\n",
        "    # 4. Add 'ticker' column - always add it after all other renames to ensure it's set correctly\n",
        "    # This ensures the global TICKER value is used, even if 'Ticker' column existed and was renamed.\n",
        "    df_daily[\"ticker\"] = TICKER\n",
        "\n",
        "    # 5. Select only the final columns we want for the database insertion\n",
        "    final_cols = [\"ticker\", \"date\", \"open\", \"high\", \"low\", \"close\", \"adj_close\", \"volume\"]\n",
        "    # Ensure all final_cols exist in df_daily before selecting, or handle missing ones if necessary\n",
        "    # For robustness, filter final_cols against actual df_daily columns\n",
        "    existing_final_cols = [col for col in final_cols if col in df_daily.columns]\n",
        "\n",
        "    if len(existing_final_cols) < len(final_cols):\n",
        "        print(f\"‚ö†Ô∏è Warning: Some expected final columns are missing. Missing: {list(set(final_cols) - set(existing_final_cols))}\")\n",
        "\n",
        "    df_daily_out = df_daily[existing_final_cols].copy()\n",
        "\n",
        "    # --- End of Robust Column Handling ---\n",
        "\n",
        "    insert_sql = text(\"\"\"\n",
        "        INSERT INTO raw_daily_prices\n",
        "            (ticker, date, open, high, low, close, adj_close, volume)\n",
        "        VALUES\n",
        "            (:ticker, :date, :open, :high, :low, :close, :adj_close, :volume)\n",
        "        ON CONFLICT (ticker, date) DO NOTHING;\n",
        "    \"\"\")\n",
        "\n",
        "\n",
        "    rows = df_daily_out.to_dict(orient=\"records\")\n",
        "    inserted = 0\n",
        "    with engine.begin() as conn:\n",
        "        for row in rows:\n",
        "            # Convert types for safety\n",
        "            params = {\n",
        "                \"ticker\": row[\"ticker\"], # 'ticker' should now always be present\n",
        "                \"date\": pd.to_datetime(row[\"date\"]).date(), # 'date' should now always be present\n",
        "                \"open\": float(row[\"open\"]) if pd.notna(row[\"open\"]) else None,\n",
        "                \"high\": float(row[\"high\"]) if pd.notna(row[\"high\"]) else None,\n",
        "                \"low\": float(row[\"low\"]) if pd.notna(row[\"low\"]) else None,\n",
        "                \"close\": float(row[\"close\"]) if pd.notna(row[\"close\"]) else None,\n",
        "                \"adj_close\": float(row[\"adj_close\"]) if pd.notna(row[\"adj_close\"]) else None,\n",
        "                \"volume\": int(row[\"volume\"]) if pd.notna(row[\"volume\"]) else None,\n",
        "            }\n",
        "            result = conn.execute(insert_sql, params)\n",
        "            # result.rowcount isn't reliable with ON CONFLICT, so we don't count precisely\n",
        "            inserted += 1\n",
        "\n",
        "    print(f\"‚úÖ Backfilled daily data (~{len(rows)} rows processed).\")\n"
      ],
      "metadata": {
        "id": "TXbIpmtlUpti"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "###\n",
        "### Hourly Data Extract Function Definition - one time for historic data\n",
        "- This Function call will be commented once the past data is loaded.\n",
        "- New hourly data will"
      ],
      "metadata": {
        "id": "BxjsH-45QBNr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from datetime import datetime, timedelta, timezone\n",
        "import pandas as pd\n",
        "import yfinance as yf\n",
        "from sqlalchemy import create_engine\n",
        "\n",
        "# make sure these exist globally:\n",
        "# DATABASE_URL, engine, TICKER, HOURLY_INTERVAL\n",
        "\n",
        "def fetch_latest_hourly(days_back: int = 700):\n",
        "    \"\"\"\n",
        "    Fetch recent HOURLY data from Yahoo Finance and append to raw_hourly_prices.\n",
        "    Handles MultiIndex columns from yfinance and avoids manual connection issues\n",
        "    by using pandas.to_sql().\n",
        "    \"\"\"\n",
        "\n",
        "    end_dt = datetime.now(timezone.utc)\n",
        "    start_dt = end_dt - timedelta(days=days_back)\n",
        "\n",
        "    print(\n",
        "        f\"üì• Downloading hourly ({HOURLY_INTERVAL}) data for {TICKER} \"\n",
        "        f\"from {start_dt} to {end_dt}...\"\n",
        "    )\n",
        "\n",
        "    df_hourly = yf.download(\n",
        "        TICKER,\n",
        "        start=start_dt,\n",
        "        end=end_dt,\n",
        "        interval=HOURLY_INTERVAL,\n",
        "        auto_adjust=False,\n",
        "        prepost=False,\n",
        "    )\n",
        "\n",
        "    if df_hourly.empty:\n",
        "        print(\"‚ö†Ô∏è No hourly data returned from Yahoo.\")\n",
        "        return\n",
        "\n",
        "    # ---------- Use index as timestamp ----------\n",
        "    df_hourly[\"ts\"] = df_hourly.index\n",
        "\n",
        "    # Reset index so ts is a normal column\n",
        "    df_hourly = df_hourly.reset_index(drop=True)\n",
        "\n",
        "    print(\"Raw columns from yfinance:\", df_hourly.columns.tolist())\n",
        "\n",
        "    # ---------- Flatten MultiIndex columns, if any ----------\n",
        "    if isinstance(df_hourly.columns, pd.MultiIndex):\n",
        "        flat_cols = []\n",
        "        for col in df_hourly.columns:\n",
        "            # col is like ('Open', 'TCS.NS') or ('ts', '')\n",
        "            if isinstance(col, tuple):\n",
        "                # take the first non-empty part\n",
        "                name = col[0] if col[0] not in (None, \"\", \" \") else col[1]\n",
        "            else:\n",
        "                name = col\n",
        "            flat_cols.append(str(name))\n",
        "        df_hourly.columns = flat_cols\n",
        "        print(\"Flattened columns:\", df_hourly.columns.tolist())\n",
        "\n",
        "    # At this point we expect something like:\n",
        "    # ['Open', 'High', 'Low', 'Close', 'Adj Close', 'Volume', 'ts']\n",
        "\n",
        "    # ---------- Ensure Adj Close exists ----------\n",
        "    if \"Adj Close\" not in df_hourly.columns and \"Adj close\" not in df_hourly.columns.lower():\n",
        "        if \"Close\" in df_hourly.columns:\n",
        "            print(\"‚ÑπÔ∏è 'Adj Close' missing, using 'Close' as proxy.\")\n",
        "            df_hourly[\"Adj Close\"] = df_hourly[\"Close\"]\n",
        "        else:\n",
        "            raise ValueError(f\"No 'Adj Close' or 'Close' in columns: {df_hourly.columns}\")\n",
        "\n",
        "    # Some versions may have 'Adj Close' vs 'Adj close'; normalize name\n",
        "    if \"Adj close\" in df_hourly.columns:\n",
        "        df_hourly.rename(columns={\"Adj close\": \"Adj Close\"}, inplace=True)\n",
        "\n",
        "    # ---------- Build clean output DataFrame ----------\n",
        "    required_cols = [\"ts\", \"Open\", \"High\", \"Low\", \"Close\", \"Adj Close\", \"Volume\"]\n",
        "    missing = [c for c in required_cols if c not in df_hourly.columns]\n",
        "    if missing:\n",
        "        raise ValueError(f\"Missing required columns {missing} in df_hourly: {df_hourly.columns}\")\n",
        "\n",
        "    df_hourly_out = df_hourly[required_cols].copy()\n",
        "    df_hourly_out.rename(\n",
        "        columns={\n",
        "            \"Open\": \"open\",\n",
        "            \"High\": \"high\",\n",
        "            \"Low\": \"low\",\n",
        "            \"Close\": \"close\",\n",
        "            \"Adj Close\": \"adj_close\",\n",
        "            \"Volume\": \"volume\",\n",
        "        },\n",
        "        inplace=True,\n",
        "    )\n",
        "\n",
        "    df_hourly_out[\"ticker\"] = TICKER\n",
        "    df_hourly_out[\"interval\"] = HOURLY_INTERVAL\n",
        "\n",
        "    # ---------- Normalize ts to proper datetime ----------\n",
        "    df_hourly_out[\"ts\"] = pd.to_datetime(df_hourly_out[\"ts\"], errors=\"coerce\")\n",
        "    df_hourly_out = df_hourly_out[df_hourly_out[\"ts\"].notna()]\n",
        "\n",
        "    print(\"Sample of cleaned hourly data:\")\n",
        "    print(df_hourly_out.head())\n",
        "\n",
        "    # ---------- Append to DB using to_sql (no manual connection handling) ----------\n",
        "    df_hourly_out.to_sql(\n",
        "        \"raw_hourly_prices\",\n",
        "        con=engine,\n",
        "        if_exists=\"append\",   # append new rows\n",
        "        index=False,\n",
        "        method=\"multi\",\n",
        "        chunksize=1000,\n",
        "    )\n",
        "\n",
        "    print(f\"‚úÖ Hourly ingestion done. Rows appended: {len(df_hourly_out)}\")\n"
      ],
      "metadata": {
        "id": "r4ePhNoSUv90"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ===================== QUICK MANUAL TEST =====================\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "\n",
        "    # 1) Ensure tables exist in your cloud Postgres\n",
        "    # Commented this part as tables are created one time. Uncomment if executing this for the first time\n",
        "    #init_tables()\n",
        "\n",
        "\n",
        "    # 2) One-time: backfill ~10 years of daily data\n",
        "    # Commenting this after one time data load for ten years\n",
        "    # backfill_daily(years_back=10)\n",
        "\n",
        "    # 3) Reusable: fetch recent hourly data (last 600 days)\n",
        "    # Commenting this after one time hourly data load for past dates\n",
        "    # fetch_latest_hourly(days_back=600)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XHflnHvdVFLu",
        "outputId": "ca6cc4d9-05f7-4fc5-b6c9-3acfe7902949"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "üì• Downloading hourly (60m) data for TCS.NS from 2024-04-05 17:34:22.362038+00:00 to 2025-11-26 17:34:22.362038+00:00...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r[*********************100%***********************]  1 of 1 completed\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Raw columns from yfinance: [('Adj Close', 'TCS.NS'), ('Close', 'TCS.NS'), ('High', 'TCS.NS'), ('Low', 'TCS.NS'), ('Open', 'TCS.NS'), ('Volume', 'TCS.NS'), ('ts', '')]\n",
            "Flattened columns: ['Adj Close', 'Close', 'High', 'Low', 'Open', 'Volume', 'ts']\n",
            "Sample of cleaned hourly data:\n",
            "                         ts         open         high          low  \\\n",
            "0 2024-04-08 03:45:00+00:00  3989.000000  3995.000000  3978.050049   \n",
            "1 2024-04-08 04:45:00+00:00  3985.449951  3998.949951  3966.500000   \n",
            "2 2024-04-08 05:45:00+00:00  3984.050049  4017.949951  3983.449951   \n",
            "3 2024-04-08 06:45:00+00:00  4016.000000  4025.949951  4007.149902   \n",
            "4 2024-04-08 07:45:00+00:00  4014.899902  4031.949951  4005.600098   \n",
            "\n",
            "         close    adj_close  volume  ticker interval  \n",
            "0  3985.000000  3985.000000       0  TCS.NS      60m  \n",
            "1  3984.050049  3984.050049  256214  TCS.NS      60m  \n",
            "2  4016.000000  4016.000000  365476  TCS.NS      60m  \n",
            "3  4014.899902  4014.899902  468619  TCS.NS      60m  \n",
            "4  4010.600098  4010.600098  363737  TCS.NS      60m  \n",
            "‚úÖ Hourly ingestion done. Rows appended: 2821\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "###\n",
        "### Hourly Data Extract - for daily excution via scheduler\n",
        "- This will be scheduled on daily basis to execute once a day to predict the clsing price of the day\n",
        "- Also, This part will be used to update last closing amount to daily table too"
      ],
      "metadata": {
        "id": "z8Dsj_McSbPf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# =====================  HOURLY DATA FETCH for the EXECUTION DATE =====================\n",
        "\n",
        "def fetch_hourly_today():\n",
        "    \"\"\"\n",
        "    Fetch ONLY today's hourly data for TICKER and append to raw_hourly_prices.\n",
        "    Safe to run multiple times a day (will re-append today's bars).\n",
        "    \"\"\"\n",
        "\n",
        "    print(f\"üì• Downloading today's hourly data for {TICKER} ({HOURLY_INTERVAL})...\")\n",
        "\n",
        "    # period='1d' is the simplest way to ask \"today's intraday\"\n",
        "    df_hourly = yf.download(\n",
        "        TICKER,\n",
        "        period=\"1d\",\n",
        "        interval=HOURLY_INTERVAL,\n",
        "        auto_adjust=False,\n",
        "        prepost=False,\n",
        "    )\n",
        "\n",
        "    if df_hourly.empty:\n",
        "        print(\"‚ö†Ô∏è No hourly data returned for today.\")\n",
        "        return\n",
        "\n",
        "    # Use index as timestamp\n",
        "    df_hourly[\"ts\"] = df_hourly.index\n",
        "    df_hourly = df_hourly.reset_index(drop=True)\n",
        "\n",
        "    # Flatten possible MultiIndex columns\n",
        "    if isinstance(df_hourly.columns, pd.MultiIndex):\n",
        "        flat_cols = []\n",
        "        for col in df_hourly.columns:\n",
        "            if isinstance(col, tuple):\n",
        "                name = col[0] if col[0] else col[1]\n",
        "            else:\n",
        "                name = col\n",
        "            flat_cols.append(str(name))\n",
        "        df_hourly.columns = flat_cols\n",
        "\n",
        "    # Ensure Adj Close column exists\n",
        "    if \"Adj Close\" not in df_hourly.columns:\n",
        "        print(\"‚ÑπÔ∏è 'Adj Close' missing, using 'Close' as proxy.\")\n",
        "        df_hourly[\"Adj Close\"] = df_hourly[\"Close\"]\n",
        "\n",
        "    df_out = df_hourly[[\"ts\", \"Open\", \"High\", \"Low\", \"Close\", \"Adj Close\", \"Volume\"]].copy()\n",
        "    df_out.rename(\n",
        "        columns={\n",
        "            \"Open\": \"open\",\n",
        "            \"High\": \"high\",\n",
        "            \"Low\": \"low\",\n",
        "            \"Close\": \"close\",\n",
        "            \"Adj Close\": \"adj_close\",\n",
        "            \"Volume\": \"volume\",\n",
        "        },\n",
        "        inplace=True,\n",
        "    )\n",
        "\n",
        "    df_out[\"ticker\"] = TICKER\n",
        "    df_out[\"interval\"] = HOURLY_INTERVAL\n",
        "\n",
        "    # Normalize ts\n",
        "    df_out[\"ts\"] = pd.to_datetime(df_out[\"ts\"], errors=\"coerce\")\n",
        "    df_out = df_out[df_out[\"ts\"].notna()]\n",
        "\n",
        "    print(\"Sample hourly rows to insert:\")\n",
        "    print(df_out.head())\n",
        "\n",
        "    # Append to DB (no manual connection headaches)\n",
        "    df_out.to_sql(\n",
        "        \"raw_hourly_prices\",\n",
        "        con=engine,\n",
        "        if_exists=\"append\",\n",
        "        index=False,\n",
        "        method=\"multi\",\n",
        "        chunksize=1000,\n",
        "    )\n",
        "\n",
        "    print(f\"‚úÖ Hourly ingestion (today) done. Rows appended: {len(df_out)}\")\n"
      ],
      "metadata": {
        "id": "EjXYUSJBTKHF"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# =====================  APPENDING DAILY from HOURLY for LATEST DATE =====================\n",
        "\n",
        "from sqlalchemy import text\n",
        "\n",
        "def append_daily_from_hourly():\n",
        "    \"\"\"\n",
        "    For any calendar dates that exist in raw_hourly_prices but not yet in\n",
        "    raw_daily_prices, aggregate hourly -> daily OHLCV and insert as new rows.\n",
        "\n",
        "    - open  = first hourly open of the day\n",
        "    - high  = max hourly high of the day\n",
        "    - low   = min hourly low of the day\n",
        "    - close = last hourly close of the day\n",
        "    - adj_close = last hourly adj_close of the day\n",
        "    - volume = sum of hourly volumes\n",
        "    \"\"\"\n",
        "\n",
        "    # 1) Get the latest daily date we already have\n",
        "    with engine.begin() as conn:\n",
        "        last_daily_date = conn.execute(\n",
        "            text(\"\"\"\n",
        "                SELECT MAX(date)\n",
        "                FROM raw_daily_prices\n",
        "                WHERE ticker = :ticker\n",
        "            \"\"\"),\n",
        "            {\"ticker\": TICKER},\n",
        "        ).scalar()\n",
        "\n",
        "    print(\"Last daily date in table:\", last_daily_date)\n",
        "\n",
        "    # 2) Load hourly data newer than that date (or all if None)\n",
        "    with engine.begin() as conn:\n",
        "        if last_daily_date is None:\n",
        "            hourly_df = pd.read_sql(\n",
        "                text(\"\"\"\n",
        "                    SELECT ts, open, high, low, close, adj_close, volume\n",
        "                    FROM raw_hourly_prices\n",
        "                    WHERE ticker = :ticker\n",
        "                \"\"\"),\n",
        "                conn,\n",
        "                params={\"ticker\": TICKER},\n",
        "            )\n",
        "        else:\n",
        "            hourly_df = pd.read_sql(\n",
        "                text(\"\"\"\n",
        "                    SELECT ts, open, high, low, close, adj_close, volume\n",
        "                    FROM raw_hourly_prices\n",
        "                    WHERE ticker = :ticker\n",
        "                      AND ts::date > :last_date\n",
        "                \"\"\"),\n",
        "                conn,\n",
        "                params={\"ticker\": TICKER, \"last_date\": last_daily_date},\n",
        "            )\n",
        "\n",
        "    if hourly_df.empty:\n",
        "        print(\"‚ÑπÔ∏è No new hourly data beyond last daily date. Nothing to aggregate.\")\n",
        "        return\n",
        "\n",
        "    # 3) Convert ts to datetime and derive calendar date\n",
        "    hourly_df[\"ts\"] = pd.to_datetime(hourly_df[\"ts\"], errors=\"coerce\")\n",
        "    hourly_df = hourly_df[hourly_df[\"ts\"].notna()]\n",
        "\n",
        "    # drop tz info for date extraction, if present\n",
        "    if hourly_df[\"ts\"].dt.tz is not None:\n",
        "        hourly_df[\"ts\"] = hourly_df[\"ts\"].dt.tz_convert(None)\n",
        "\n",
        "    hourly_df[\"date\"] = hourly_df[\"ts\"].dt.date\n",
        "\n",
        "    # 4) Aggregate hourly -> daily OHLCV\n",
        "    agg = hourly_df.sort_values(\"ts\").groupby(\"date\").agg(\n",
        "        open=(\"open\", \"first\"),\n",
        "        high=(\"high\", \"max\"),\n",
        "        low=(\"low\", \"min\"),\n",
        "        close=(\"close\", \"last\"),\n",
        "        adj_close=(\"adj_close\", \"last\"),\n",
        "        volume=(\"volume\", \"sum\"),\n",
        "    ).reset_index()\n",
        "\n",
        "    print(\"Daily aggregates to insert:\")\n",
        "    print(agg.head())\n",
        "\n",
        "    # 5) Insert aggregated rows into raw_daily_prices\n",
        "    insert_sql = text(\"\"\"\n",
        "        INSERT INTO raw_daily_prices\n",
        "            (ticker, date, open, high, low, close, adj_close, volume)\n",
        "        VALUES\n",
        "            (:ticker, :date, :open, :high, :low, :close, :adj_close, :volume)\n",
        "        ON CONFLICT (ticker, date) DO NOTHING;\n",
        "    \"\"\")\n",
        "\n",
        "    with engine.begin() as conn:\n",
        "        inserted = 0\n",
        "        for _, row in agg.iterrows():\n",
        "            params = {\n",
        "                \"ticker\": TICKER,\n",
        "                \"date\": row[\"date\"],\n",
        "                \"open\": float(row[\"open\"]) if pd.notna(row[\"open\"]) else None,\n",
        "                \"high\": float(row[\"high\"]) if pd.notna(row[\"high\"]) else None,\n",
        "                \"low\": float(row[\"low\"]) if pd.notna(row[\"low\"]) else None,\n",
        "                \"close\": float(row[\"close\"]) if pd.notna(row[\"close\"]) else None,\n",
        "                \"adj_close\": float(row[\"adj_close\"]) if pd.notna(row[\"adj_close\"]) else None,\n",
        "                \"volume\": int(row[\"volume\"]) if pd.notna(row[\"volume\"]) else None,\n",
        "            }\n",
        "            conn.execute(insert_sql, params)\n",
        "            inserted += 1\n",
        "\n",
        "    print(f\"‚úÖ Aggregated and inserted {inserted} new daily rows from hourly data.\")\n"
      ],
      "metadata": {
        "id": "OlY43OlIUfKC"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# =====================  CAlling the hourly fetch function  =====================\n",
        "\n",
        "fetch_hourly_today()"
      ],
      "metadata": {
        "id": "8lKLDFXiVT85",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e4aafe53-bff7-4af7-f8eb-0338304b02cd"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "üì• Downloading today's hourly data for TCS.NS (60m)...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r[*********************100%***********************]  1 of 1 completed\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Sample hourly rows to insert:\n",
            "                         ts         open         high          low  \\\n",
            "0 2025-11-27 03:45:00+00:00  3178.000000  3178.899902  3148.500000   \n",
            "1 2025-11-27 04:45:00+00:00  3164.399902  3164.600098  3155.100098   \n",
            "2 2025-11-27 05:45:00+00:00  3158.699951  3159.199951  3145.000000   \n",
            "3 2025-11-27 06:45:00+00:00  3145.699951  3146.000000  3129.000000   \n",
            "4 2025-11-27 07:45:00+00:00  3130.000000  3133.000000  3125.000000   \n",
            "\n",
            "         close    adj_close  volume  ticker interval  \n",
            "0  3164.600098  3164.600098  468909  TCS.NS      60m  \n",
            "1  3157.500000  3157.500000  302753  TCS.NS      60m  \n",
            "2  3145.699951  3145.699951  251005  TCS.NS      60m  \n",
            "3  3130.000000  3130.000000  388684  TCS.NS      60m  \n",
            "4  3127.699951  3127.699951  296511  TCS.NS      60m  \n",
            "‚úÖ Hourly ingestion (today) done. Rows appended: 7\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# =====================  CAlling the daily append function  =====================\n",
        "\n",
        "append_daily_from_hourly()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xwEi8-TqU7aV",
        "outputId": "69a6d011-fd24-4b5d-b4a7-1eaafc714b1e"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Last daily date in table: 2025-11-27\n",
            "‚ÑπÔ∏è No new hourly data beyond last daily date. Nothing to aggregate.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "###\n",
        "### Quality Checks on both Hourly and Daily Tables"
      ],
      "metadata": {
        "id": "PdkJ2yfxJaxS"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "####\n",
        "#### Generic QC Runner"
      ],
      "metadata": {
        "id": "r9TcjTDVJfhd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ======================= GENERIC QC RUNNER ===========================\n",
        "\n",
        "from sqlalchemy import text\n",
        "import pandas as pd\n",
        "from datetime import date, timedelta\n",
        "\n",
        "def run_qc_checks(engine, checks, scope_params=None):\n",
        "    \"\"\"\n",
        "    Generic QC runner:\n",
        "    - checks: list of dicts {name, sql, level}\n",
        "    - scope_params: extra params injected into SQL (e.g., ticker, date_from)\n",
        "    \"\"\"\n",
        "    results = []\n",
        "\n",
        "    with engine.begin() as conn:\n",
        "        for chk in checks:\n",
        "            params = scope_params or {}\n",
        "            row = conn.execute(text(chk[\"sql\"]), params).fetchone()\n",
        "            value = row[0] if row is not None else None\n",
        "\n",
        "            results.append(\n",
        "                {\n",
        "                    \"name\": chk[\"name\"],\n",
        "                    \"level\": chk.get(\"level\", \"info\"),\n",
        "                    \"value\": value,\n",
        "                }\n",
        "            )\n",
        "\n",
        "    return pd.DataFrame(results)\n"
      ],
      "metadata": {
        "id": "fpszbrRgJjXT"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "####\n",
        "#### Hourly Quality Checks"
      ],
      "metadata": {
        "id": "X5XGh5JIKPAf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ========================== HOURLY QC =============================\n",
        "\n",
        "HOURLY_QC_CHECKS = [\n",
        "    {\n",
        "        \"name\": \"hourly_duplicate_keys\",\n",
        "        \"level\": \"error\",\n",
        "        \"sql\": \"\"\"\n",
        "            SELECT COUNT(*) FROM (\n",
        "                SELECT ticker, ts, interval, COUNT(*) AS c\n",
        "                FROM raw_hourly_prices\n",
        "                WHERE ticker = :ticker\n",
        "                {date_filter}\n",
        "                GROUP BY ticker, ts, interval\n",
        "                HAVING COUNT(*) > 1\n",
        "            ) t;\n",
        "        \"\"\",\n",
        "    },\n",
        "    {\n",
        "        \"name\": \"hourly_null_prices\",\n",
        "        \"level\": \"error\",\n",
        "        \"sql\": \"\"\"\n",
        "            SELECT COUNT(*)\n",
        "            FROM raw_hourly_prices\n",
        "            WHERE ticker = :ticker\n",
        "              {date_filter}\n",
        "              AND (\n",
        "                open IS NULL OR close IS NULL OR ts IS NULL\n",
        "              );\n",
        "        \"\"\",\n",
        "    },\n",
        "    {\n",
        "        \"name\": \"hourly_future_timestamps\",\n",
        "        \"level\": \"warn\",\n",
        "        \"sql\": \"\"\"\n",
        "            SELECT COUNT(*)\n",
        "            FROM raw_hourly_prices\n",
        "            WHERE ticker = :ticker\n",
        "              {date_filter}\n",
        "              AND ts > NOW() + INTERVAL '5 minutes';\n",
        "        \"\"\",\n",
        "    },\n",
        "]\n"
      ],
      "metadata": {
        "id": "GeKVjfngKSk5"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "####\n",
        "#### Building Date Filter Helper - to allow full vs recent QC\n"
      ],
      "metadata": {
        "id": "MuRMgz1MLHD0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def build_date_filter(column_name: str, date_from=None):\n",
        "    if date_from is None:\n",
        "        return \"\", {}\n",
        "    else:\n",
        "        return f\"AND {column_name}::date >= :date_from\", {\"date_from\": date_from}\n"
      ],
      "metadata": {
        "id": "szZYBNYHLfU-"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "####\n",
        "#### Historical Hourly QC"
      ],
      "metadata": {
        "id": "DmCu1pQlLgcU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def qc_hourly_full(engine, ticker):\n",
        "    checks_sql = []\n",
        "    for chk in HOURLY_QC_CHECKS:\n",
        "        date_filter, extra_params = build_date_filter(\"ts\", date_from=None)\n",
        "        sql = chk[\"sql\"].format(date_filter=date_filter)\n",
        "        checks_sql.append({**chk, \"sql\": sql})\n",
        "\n",
        "    df = run_qc_checks(\n",
        "        engine,\n",
        "        checks_sql,\n",
        "        scope_params={\"ticker\": ticker},\n",
        "    )\n",
        "    print(\"Full hourly QC:\")\n",
        "    print(df)\n",
        "    return df\n"
      ],
      "metadata": {
        "id": "TnWvK7aILk9X"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "####\n",
        "#### Daily/Incremental Hourly QC"
      ],
      "metadata": {
        "id": "wO7L2oQOLnmX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def qc_hourly_incremental(engine, ticker, days_back=1):\n",
        "    date_from = date.today() - timedelta(days=days_back)\n",
        "\n",
        "    checks_sql = []\n",
        "    for chk in HOURLY_QC_CHECKS:\n",
        "        date_filter, extra_params = build_date_filter(\"ts\", date_from=date_from)\n",
        "        sql = chk[\"sql\"].format(date_filter=date_filter)\n",
        "        checks_sql.append({**chk, \"sql\": sql})\n",
        "\n",
        "    params = {\"ticker\": ticker, \"date_from\": date_from}\n",
        "    df = run_qc_checks(engine, checks_sql, scope_params=params)\n",
        "    print(f\"Incremental hourly QC from {date_from}:\")\n",
        "    print(df)\n",
        "    return df\n"
      ],
      "metadata": {
        "id": "T5AWbuJ-LrBp"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "###\n",
        "### Daily Quality Checks"
      ],
      "metadata": {
        "id": "xZtsqm_WLt0k"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "DAILY_QC_CHECKS = [\n",
        "    {\n",
        "        \"name\": \"daily_duplicate_dates\",\n",
        "        \"level\": \"error\",\n",
        "        \"sql\": \"\"\"\n",
        "            SELECT COUNT(*) FROM (\n",
        "                SELECT ticker, date, COUNT(*) AS c\n",
        "                FROM raw_daily_prices\n",
        "                WHERE ticker = :ticker\n",
        "                {date_filter}\n",
        "                GROUP BY ticker, date\n",
        "                HAVING COUNT(*) > 1\n",
        "            ) t;\n",
        "        \"\"\",\n",
        "    },\n",
        "    {\n",
        "      \"name\": \"daily_missing_prices\",\n",
        "      \"level\": \"error\",\n",
        "      \"sql\": \"\"\"\n",
        "          SELECT COUNT(*)\n",
        "          FROM raw_daily_prices\n",
        "          WHERE ticker = :ticker\n",
        "            {date_filter}\n",
        "            AND (\n",
        "              open IS NULL OR close IS NULL\n",
        "            );\n",
        "      \"\"\"\n",
        "    }\n",
        "]\n"
      ],
      "metadata": {
        "id": "RmxNKcifLwzv"
      },
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "####\n",
        "#### Daily QC - Full and Recent"
      ],
      "metadata": {
        "id": "0ytY-f76MLnl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ====================== Daily QC - Full Data =======================\n",
        "\n",
        "def qc_daily_full(engine, ticker):\n",
        "    checks_sql = []\n",
        "    for chk in DAILY_QC_CHECKS:\n",
        "        date_filter, _ = build_date_filter(\"date\", date_from=None)\n",
        "        sql = chk[\"sql\"].format(date_filter=date_filter)\n",
        "        checks_sql.append({**chk, \"sql\": sql})\n",
        "\n",
        "    df = run_qc_checks(engine, checks_sql, scope_params={\"ticker\": ticker})\n",
        "    print(\"Full daily QC:\")\n",
        "    print(df)\n",
        "    return df\n",
        "\n",
        "\n",
        "# ====================== Daily QC - Incremental Data ====================\n",
        "\n",
        "def qc_daily_incremental(engine, ticker, days_back=1):\n",
        "    date_from = date.today() - timedelta(days=days_back)\n",
        "\n",
        "    checks_sql = []\n",
        "    for chk in DAILY_QC_CHECKS:\n",
        "        date_filter, _ = build_date_filter(\"date\", date_from=date_from)\n",
        "        sql = chk[\"sql\"].format(date_filter=date_filter)\n",
        "        checks_sql.append({**chk, \"sql\": sql})\n",
        "\n",
        "    params = {\"ticker\": ticker, \"date_from\": date_from}\n",
        "    df = run_qc_checks(engine, checks_sql, scope_params=params)\n",
        "    print(f\"Incremental daily QC from {date_from}:\")\n",
        "    print(df)\n",
        "    return df\n"
      ],
      "metadata": {
        "id": "3IH_czASMT5i"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "###\n",
        "### Executing Quality Check steps for Historic Load"
      ],
      "metadata": {
        "id": "kYWuy910MqpQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Commenting this part as historic QC was needed only once, but can be reused later if required\n",
        "\n",
        "# qc_hourly = qc_hourly_full(engine, 'TCS.NS')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CxpdRvMPMyPN",
        "outputId": "72209c73-6ba1-4cfa-d3bb-429b713fea8a"
      },
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Full hourly QC:\n",
            "                       name  level  value\n",
            "0     hourly_duplicate_keys  error      0\n",
            "1        hourly_null_prices  error      0\n",
            "2  hourly_future_timestamps   warn      0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Commenting this part as historic QC was needed only once, but can be reused later if required\n",
        "\n",
        "# qc_daily = qc_daily_full(engine, 'TCS.NS')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "COCGCK6VPWNy",
        "outputId": "fd2e38e3-8f71-40df-de58-66fd0fcc0239"
      },
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Full daily QC:\n",
            "                    name  level  value\n",
            "0  daily_duplicate_dates  error      0\n",
            "1   daily_missing_prices  error      0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "###\n",
        "### Executing Quality Check steps for Incremental Load"
      ],
      "metadata": {
        "id": "7n5NnlDUM3bw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def run_ingestion_pipeline():\n",
        "    qc_hourly = qc_hourly_incremental(engine, TICKER, days_back=1)\n",
        "\n",
        "    qc_daily = qc_daily_incremental(engine, TICKER, days_back=1)\n"
      ],
      "metadata": {
        "id": "v8Vx4z5GNGEu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "###\n",
        "### Quality Checks Actions"
      ],
      "metadata": {
        "id": "43fT71kkSdZ3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ========================== Actions on Hourly QC results ==============================\n",
        "\n",
        "from sqlalchemy import text\n",
        "\n",
        "def apply_hourly_qc_actions(engine, qc_df):\n",
        "    \"\"\"\n",
        "    Deletes invalid HOURLY records based on QC findings:\n",
        "    - Rows with NULL price/volume\n",
        "    - Duplicate (ticker, ts, interval) rows (keeps one)\n",
        "    \"\"\"\n",
        "\n",
        "    with engine.begin() as conn:\n",
        "\n",
        "        # ---------- 1. Delete NULL / missing rows ----------\n",
        "        try:\n",
        "            null_issues = int(\n",
        "                qc_df.loc[\n",
        "                    qc_df[\"name\"] == \"hourly_null_or_missing_values\", \"value\"\n",
        "                ].iloc[0]\n",
        "            )\n",
        "        except IndexError:\n",
        "            null_issues = 0\n",
        "\n",
        "        if null_issues > 0:\n",
        "            print(f\"üßπ Deleting {null_issues} hourly rows with NULL values\")\n",
        "\n",
        "            conn.execute(text(\"\"\"\n",
        "                DELETE FROM raw_hourly_prices\n",
        "                WHERE ticker = :ticker\n",
        "                  AND (\n",
        "                        open IS NULL\n",
        "                     OR high IS NULL\n",
        "                     OR low IS NULL\n",
        "                     OR close IS NULL\n",
        "                     OR adj_close IS NULL\n",
        "                     OR volume IS NULL\n",
        "                  );\n",
        "            \"\"\"), {\"ticker\": TICKER})\n",
        "\n",
        "        else:\n",
        "          print(\"‚úÖ No missing hourly prices found\")\n",
        "\n",
        "        # ---------- 2. Delete duplicate key rows ----------\n",
        "        try:\n",
        "            dup_issues = int(\n",
        "                qc_df.loc[\n",
        "                    qc_df[\"name\"] == \"hourly_duplicate_keys\", \"value\"\n",
        "                ].iloc[0]\n",
        "            )\n",
        "        except IndexError:\n",
        "            dup_issues = 0\n",
        "\n",
        "        if dup_issues > 0:\n",
        "            print(f\"üßπ Removing duplicate hourly rows (keeping one per key)\")\n",
        "\n",
        "            # Use ctid trick to delete \"older\" duplicates\n",
        "            conn.execute(text(\"\"\"\n",
        "                DELETE FROM raw_hourly_prices a\n",
        "                USING raw_hourly_prices b\n",
        "                WHERE a.ctid < b.ctid\n",
        "                  AND a.ticker  = b.ticker\n",
        "                  AND a.ts      = b.ts\n",
        "                  AND a.interval = b.interval;\n",
        "            \"\"\"))\n",
        "        else:\n",
        "          print(\"‚úÖ No duplicate hourly records found\")\n"
      ],
      "metadata": {
        "id": "6O2Ku67fSgP6"
      },
      "execution_count": 31,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ================================ Actions on Daily QC results =========================\n",
        "\n",
        "def apply_daily_qc_actions(engine, qc_df):\n",
        "    \"\"\"\n",
        "    Deletes invalid DAILY records based on QC findings:\n",
        "    - Rows with NULL price/volume\n",
        "    - Duplicate (ticker, date) rows (keeps one)\n",
        "    \"\"\"\n",
        "\n",
        "    with engine.begin() as conn:\n",
        "\n",
        "        # ---------- 1. Delete NULL / missing rows ----------\n",
        "        try:\n",
        "            null_issues = int(\n",
        "                qc_df.loc[\n",
        "                    qc_df[\"name\"] == \"daily_null_or_missing_values\", \"value\"\n",
        "                ].iloc[0]\n",
        "            )\n",
        "        except IndexError:\n",
        "            null_issues = 0\n",
        "\n",
        "        if null_issues > 0:\n",
        "            print(f\"üßπ Deleting {null_issues} daily rows with NULL values\")\n",
        "\n",
        "            conn.execute(text(\"\"\"\n",
        "                DELETE FROM raw_daily_prices\n",
        "                WHERE ticker = :ticker\n",
        "                  AND (\n",
        "                        open IS NULL\n",
        "                     OR high IS NULL\n",
        "                     OR low IS NULL\n",
        "                     OR close IS NULL\n",
        "                     OR adj_close IS NULL\n",
        "                     OR volume IS NULL\n",
        "                  );\n",
        "            \"\"\"), {\"ticker\": TICKER})\n",
        "\n",
        "        else:\n",
        "          print(\"‚úÖ No missing daily prices found\")\n",
        "\n",
        "        # ---------- 2. Delete duplicate (ticker, date) rows ----------\n",
        "        try:\n",
        "            dup_issues = int(\n",
        "                qc_df.loc[\n",
        "                    qc_df[\"name\"] == \"daily_duplicate_dates\", \"value\"\n",
        "                ].iloc[0]\n",
        "            )\n",
        "        except IndexError:\n",
        "            dup_issues = 0\n",
        "\n",
        "        if dup_issues > 0:\n",
        "            print(f\"üßπ Removing duplicate daily rows (keeping one per key)\")\n",
        "\n",
        "            conn.execute(text(\"\"\"\n",
        "                DELETE FROM raw_daily_prices a\n",
        "                USING raw_daily_prices b\n",
        "                WHERE a.ctid < b.ctid\n",
        "                  AND a.ticker = b.ticker\n",
        "                  AND a.date   = b.date;\n",
        "            \"\"\"))\n",
        "\n",
        "        else:\n",
        "          print(\"‚úÖ No duplicate daily records found\")\n"
      ],
      "metadata": {
        "id": "geKewTNkSn0S"
      },
      "execution_count": 30,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Calling action function\n",
        "\n",
        "apply_hourly_qc_actions(engine, qc_hourly)\n",
        "print(\"\\n\\n\")\n",
        "apply_daily_qc_actions(engine, qc_daily)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CS7PZER9TL8Q",
        "outputId": "69647369-a074-413f-aaf4-e03a35060a2b"
      },
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "‚úÖ No missing hourly prices found\n",
            "‚úÖ No duplicate hourly records found\n",
            "\n",
            "\n",
            "\n",
            "‚úÖ No missing daily prices found\n",
            "‚úÖ No duplicate daily records found\n"
          ]
        }
      ]
    }
  ]
}